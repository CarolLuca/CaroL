{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "behind the model 0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjzhCmQsZsi2YuJ2ah2KTH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolLuca/NER_Legal_Domain_RO/blob/main/Behind%20The%20Model/behind_the_model_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuBj9PIqIkWO"
      },
      "source": [
        " doc = nlp(\"This is a text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEuCXZIInCx"
      },
      "source": [
        " texts = [\"This is a text\", \"These are lots of texts\", \"...\"]\n",
        " - docs = [nlp(text) for text in texts]\n",
        " + docs = list(nlp.pipe(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCSqUHWjIoW7"
      },
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu2X5PBsIqIs"
      },
      "source": [
        "{\n",
        "  \"lang\": \"en\",\n",
        "  \"name\": \"core_web_sm\",\n",
        "  \"description\": \"Example model for spaCy\",\n",
        "  \"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaHo9KIpIr7v"
      },
      "source": [
        " nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqVVp2T6ItGv"
      },
      "source": [
        "lang = \"en\"\n",
        "pipeline = [\"tagger\", \"parser\", \"ner\"]\n",
        "data_path = \"path/to/en_core_web_sm/en_core_web_sm-2.0.0\"\n",
        "\n",
        "cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
        "nlp = cls()                             # 2. Initialize it\n",
        "for name in pipeline:\n",
        "    component = nlp.create_pipe(name)   # 3. Create the pipeline components\n",
        "    nlp.add_pipe(component)             # 4. Add the component to the pipeline\n",
        "nlp.from_disk(model_data_path)          # 5. Load in the binary data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiplPr2IuqW"
      },
      "source": [
        "doc = nlp.make_doc(\"This is a sentence\")   # create a Doc from raw text\n",
        "for name, proc in nlp.pipeline:             # iterate over components in order\n",
        "    doc = proc(doc)                         # apply each component"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9WJME-TI2a0"
      },
      "source": [
        "print(nlp.pipeline)\n",
        "# [('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>), ('ner', <spacy.pipeline.EntityRecognizer>)]\n",
        "print(nlp.pipe_names)\n",
        "# ['tagger', 'parser', 'ner']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRYPhLhNI4Wt"
      },
      "source": [
        "# Option 1: Import and initialize\n",
        "from spacy.pipeline import EntityRuler\n",
        "ruler = EntityRuler(nlp)\n",
        "nlp.add_pipe(ruler)\n",
        "\n",
        "# Option 2: Using nlp.create_pipe\n",
        "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "nlp.add_pipe(sentencizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTq3HfupI50r"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
        "nlp = English().from_disk(\"/model\", disable=[\"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX7KP9YwI_0B"
      },
      "source": [
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI5QYaVQJBr3"
      },
      "source": [
        "# 1. Use as a contextmanager\n",
        "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
        "    doc = nlp(\"I won't be tagged and parsed\")\n",
        "doc = nlp(\"I will be tagged and parsed\")\n",
        "\n",
        "# 2. Restore manually\n",
        "disabled = nlp.disable_pipes(\"ner\")\n",
        "doc = nlp(\"I won't have named entities\")\n",
        "disabled.restore()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx0aDlawJDH_"
      },
      "source": [
        "nlp.remove_pipe(\"parser\")\n",
        "nlp.rename_pipe(\"ner\", \"entityrecognizer\")\n",
        "nlp.replace_pipe(\"tagger\", my_custom_tagger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2AFtY5vJEgq"
      },
      "source": [
        "def my_component(doc):\n",
        "   # do something to the doc here\n",
        "   return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCllh5bQJFwi"
      },
      "source": [
        "nlp.add_pipe(my_component)\n",
        "nlp.add_pipe(my_component, first=True)\n",
        "nlp.add_pipe(my_component, before=\"parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzyX7S4IJHF4"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "\"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
        "based on list of single or multiple-word company names. Companies are\n",
        "labelled as ORG and their spans are merged into one token. Additionally,\n",
        "._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
        "respectively.\n",
        "\n",
        "* Custom pipeline components: https://spacy.io//usage/processing-pipelines#custom-components\n",
        "\n",
        "Compatible with: spaCy v2.0.0+\n",
        "Last tested with: v2.1.0\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    text=(\"Text to process\", \"positional\", None, str),\n",
        "    companies=(\"Names of technology companies\", \"positional\", None, str),\n",
        ")\n",
        "def main(text=\"Alphabet Inc. is the company behind Google.\", *companies):\n",
        "    # For simplicity, we start off with only the blank English Language class\n",
        "    # and no model or pre-defined pipeline loaded.\n",
        "    nlp = English()\n",
        "    if not companies:  # set default companies if none are set via args\n",
        "        companies = [\"Alphabet Inc.\", \"Google\", \"Netflix\", \"Apple\"]  # etc.\n",
        "    component = TechCompanyRecognizer(nlp, companies)  # initialise component\n",
        "    nlp.add_pipe(component, last=True)  # add last to the pipeline\n",
        "\n",
        "    doc = nlp(text)\n",
        "    print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
        "    print(\"Tokens\", [t.text for t in doc])  # company names from the list are merged\n",
        "    print(\"Doc has_tech_org\", doc._.has_tech_org)  # Doc contains tech orgs\n",
        "    print(\"Token 0 is_tech_org\", doc[0]._.is_tech_org)  # \"Alphabet Inc.\" is a tech org\n",
        "    print(\"Token 1 is_tech_org\", doc[1]._.is_tech_org)  # \"is\" is not\n",
        "    print(\"Entities\", [(e.text, e.label_) for e in doc.ents])  # all orgs are entities\n",
        "\n",
        "\n",
        "class TechCompanyRecognizer(object):\n",
        "    \"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
        "    based on list of single or multiple-word company names. Companies are\n",
        "    labelled as ORG and their spans are merged into one token. Additionally,\n",
        "    ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
        "    respectively.\"\"\"\n",
        "\n",
        "    name = \"tech_companies\"  # component name, will show up in the pipeline\n",
        "\n",
        "    def __init__(self, nlp, companies=tuple(), label=\"ORG\"):\n",
        "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
        "        to initialise the matcher with the shared vocab, get the label ID and\n",
        "        generate Doc objects as phrase match patterns.\n",
        "        \"\"\"\n",
        "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
        "\n",
        "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
        "        # so even if the list of companies is long, it's very efficient\n",
        "        patterns = [nlp(org) for org in companies]\n",
        "        self.matcher = PhraseMatcher(nlp.vocab)\n",
        "        self.matcher.add(\"TECH_ORGS\", None, *patterns)\n",
        "\n",
        "        # Register attribute on the Token. We'll be overwriting this based on\n",
        "        # the matches, so we're only setting a default value, not a getter.\n",
        "        Token.set_extension(\"is_tech_org\", default=False)\n",
        "\n",
        "        # Register attributes on Doc and Span via a getter that checks if one of\n",
        "        # the contained tokens is set to is_tech_org == True.\n",
        "        Doc.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
        "        Span.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
        "        are found. Return the Doc, so it can be processed by the next component\n",
        "        in the pipeline, if available.\n",
        "        \"\"\"\n",
        "        matches = self.matcher(doc)\n",
        "        spans = []  # keep the spans for later so we can merge them afterwards\n",
        "        for _, start, end in matches:\n",
        "            # Generate Span representing the entity & set label\n",
        "            entity = Span(doc, start, end, label=self.label)\n",
        "            spans.append(entity)\n",
        "            # Set custom attribute on each token of the entity\n",
        "            for token in entity:\n",
        "                token._.set(\"is_tech_org\", True)\n",
        "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
        "            doc.ents = list(doc.ents) + [entity]\n",
        "        for span in spans:\n",
        "            # Iterate over all spans and merge them into one token. This is done\n",
        "            # after setting the entities – otherwise, it would cause mismatched\n",
        "            # indices!\n",
        "            span.merge()\n",
        "        return doc  # don't forget to return the Doc!\n",
        "\n",
        "    def has_tech_org(self, tokens):\n",
        "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
        "        is a tech org. Since the getter is only called when we access the\n",
        "        attribute, we can refer to the Token's 'is_tech_org' attribute here,\n",
        "        which is already set in the processing step.\"\"\"\n",
        "        return any([t._.get(\"is_tech_org\") for t in tokens])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)\n",
        "\n",
        "    # Expected output:\n",
        "    # Pipeline ['tech_companies']\n",
        "    # Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
        "    # Doc has_tech_org True\n",
        "    # Token 0 is_tech_org True\n",
        "    # Token 1 is_tech_org False\n",
        "    # Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5T6a8p0JKPb"
      },
      "source": [
        "\"banana.vector\":\n",
        "\n",
        "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
        "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
        "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
        "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
        "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
        "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
        "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
        "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
        "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
        "       # ... and so on ...\n",
        "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
        "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
        "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8sFLqDxJMh4"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = nlp(\"dog cat banana afskfsd\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zLP3tXEJOhp"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
        "tokens = nlp(\"dog cat banana\")\n",
        "\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlTbbFo8JQWI"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
        "existing model or a blank model.\n",
        "\n",
        "For more details, see the documentation:\n",
        "* Training: https://spacy.io/usage/training\n",
        "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
        "\n",
        "Compatible with: spaCy v2.0.0+\n",
        "Last tested with: v2.2.4\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
        "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        ")\n",
        "def main(model=None, output_dir=None, n_iter=100):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    # only train NER\n",
        "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
        "        # show warnings for misaligned entity spans once\n",
        "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
        "\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    for text, _ in TRAIN_DATA:\n",
        "        doc = nlp(text)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        for text, _ in TRAIN_DATA:\n",
        "            doc = nlp2(text)\n",
        "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)\n",
        "\n",
        "    # Expected output:\n",
        "    # Entities [('Shaka Khan', 'PERSON')]\n",
        "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
        "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
        "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
        "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
        "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAtRBApIJUYg"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "\"\"\"Example of training an additional entity type\n",
        "\n",
        "This script shows how to add a new entity type to an existing pretrained NER\n",
        "model. To keep the example short and simple, only four sentences are provided\n",
        "as examples. In practice, you'll need many more — a few hundred would be a\n",
        "good start. You will also likely need to mix in examples of other entity\n",
        "types, which might be obtained by running the entity recognizer over unlabelled\n",
        "sentences, and adding their annotations to the training set.\n",
        "\n",
        "The actual training is performed by looping over the examples, and calling\n",
        "`nlp.entity.update()`. The `update()` method steps through the words of the\n",
        "input. At each word, it makes a prediction. It then consults the annotations\n",
        "provided on the GoldParse instance, to see whether it was right. If it was\n",
        "wrong, it adjusts its weights so that the correct action will score higher\n",
        "next time.\n",
        "\n",
        "After training your model, you can save it to a directory. We recommend\n",
        "wrapping models as Python packages, for ease of deployment.\n",
        "\n",
        "For more details, see the documentation:\n",
        "* Training: https://spacy.io/usage/training\n",
        "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
        "\n",
        "Compatible with: spaCy v2.1.0+\n",
        "Last tested with: v2.2.4\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# new entity label\n",
        "LABEL = \"ANIMAL\"\n",
        "\n",
        "# training data\n",
        "# Note: If you're using an existing model, make sure to mix in examples of\n",
        "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
        "# model might learn the new type, but \"forget\" what it previously knew.\n",
        "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"Horses are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 6, LABEL)]},\n",
        "    ),\n",
        "    (\"Do they bite?\", {\"entities\": []}),\n",
        "    (\n",
        "        \"horses are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 6, LABEL)]},\n",
        "    ),\n",
        "    (\"horses pretend to care about your feelings\", {\"entities\": [(0, 6, LABEL)]}),\n",
        "    (\n",
        "        \"they pretend to care about your feelings, those horses\",\n",
        "        {\"entities\": [(48, 54, LABEL)]},\n",
        "    ),\n",
        "    (\"horses?\", {\"entities\": [(0, 6, LABEL)]}),\n",
        "]\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        ")\n",
        "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
        "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
        "    random.seed(0)\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "    # Add entity recognizer to model if it's not in the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
        "    # Adding extraneous labels shouldn't mess anything up\n",
        "    ner.add_label(\"VEGETABLE\")\n",
        "    if model is None:\n",
        "        optimizer = nlp.begin_training()\n",
        "    else:\n",
        "        optimizer = nlp.resume_training()\n",
        "    move_names = list(ner.move_names)\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    # only train NER\n",
        "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
        "        # show warnings for misaligned entity spans once\n",
        "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
        "\n",
        "        sizes = compounding(1.0, 4.0, 1.001)\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "            losses = {}\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    test_text = \"Do you like horses?\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Entities in '%s'\" % test_text)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        # Check the classes have loaded back consistently\n",
        "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
        "        doc2 = nlp2(test_text)\n",
        "        for ent in doc2.ents:\n",
        "            print(ent.label_, ent.text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}