{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "accommodation 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcV+60IiWCGuZ+lP/GC7xt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolLuca/NER_Legal_Domain_RO/blob/main/Accommodation/accommodation_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMeROW2HZMDx"
      },
      "source": [
        "pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0w3AFlKgnUV"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"ro_core_news_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qepc2pphbX0"
      },
      "source": [
        "# Train NER from a blank spacy model\n",
        "import spacy\n",
        "\n",
        "nlp=spacy.blank(\"ro\")\n",
        "\n",
        "nlp.add_pipe(nlp.create_pipe('ner'))\n",
        "\n",
        "nlp.begin_training()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1BBrDkEheN-"
      },
      "source": [
        "# Import and load the spacy model\n",
        "import spacy\n",
        "nlp=spacy.load(\"ro_core_news_lg\") \n",
        "\n",
        "# Getting the ner component\n",
        "ner=nlp.get_pipe('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTGM0lj3hh0H"
      },
      "source": [
        "TRAIN_DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89JWhvlBhmDV"
      },
      "source": [
        "# Add the new label to ner\n",
        "ner.add_label(\"ORG\")\n",
        "ner.add_label(\"LOC\")\n",
        "ner.add_label(\"PER\")\n",
        "ner.add_label(\"LAW\")\n",
        "ner.add_label(\"TIME\")\n",
        "\n",
        "# Resume training\n",
        "optimizer = nlp.resume_training()\n",
        "move_names = list(ner.move_names)\n",
        "\n",
        "# List of pipes you want to train\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "\n",
        "# List of pipes which should remain unaffected in training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spvC72GehoyC"
      },
      "source": [
        "# Importing requirements\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# Begin training by disabling other pipeline components\n",
        "with nlp.disable_pipes(*other_pipes) :\n",
        "\n",
        "  sizes = compounding(1.0, 4.0, 1.001)\n",
        "  # Training for 30 iterations     \n",
        "  for itn in range(30):\n",
        "    # shuffle examples before training\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "    # ictionary to store losses\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      # Calling update() over the iteration\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "      print(\"Losses\", losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQcMqbxQhyd1"
      },
      "source": [
        "# Testing the NER\n",
        "\n",
        "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}